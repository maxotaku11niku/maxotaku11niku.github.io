<!DOCTYPE html>
<html lang="en-GB">
	<head>
		<title>Stuff by Max - Blog - Emulating Analogue Video</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width">
		<link rel="stylesheet" href="../stdstyle.css" />
		<link rel="shortcut icon" href="../assets/icon.png" />
	</head>
	<body>
		<div id="commonHeader" class="masterhead">
			<img src="../assets/banner.png" id="banner" title="Welcome to my website!"/>
		</div>
		<div id="belowHead">
			<div id="topnavbar" class="topnav">
				<ul>
					<li><a href="../index.html">Home</a></li>
					<li><a href="../blogtop.html">Blog</a></li>
					<li><a href="../projectstop.html">Projects</a></li>
					<li><a href="../links.html">Links</a></li>
					<li><a href="../about.html">About</a></li>
				</ul>
			</div>
			<div id="topnavbarmobile" class="topnavmobile">
				<p id="menuButton" class="closed">Menu</p>
				<ul id="menu" class="closed">
					<li><a href="../index.html">Home</a></li>
					<li><a href="../blogtop.html">Blog</a></li>
					<li><a href="../projectstop.html">Projects</a></li>
					<li><a href="../links.html">Links</a></li>
					<li><a href="../about.html">About</a></li>
				<ul>
			</div>
			<div id="mainBody" class="main">
				<div id="bodyHeader" class="pagehead">
					<h1>Blog maxBlog = new Blog();</h1>
					<h2>My blog</h2>
				</div>
				<!--HINT:nav-->
				<div id="blognav" class="postnav">
					<ul>
						<li><a href="20230120_pc98video.html">Previous post: Basic Video On PC-9801</a></li>
						<li><a href="">No next post</a></li>
					</ul>
				</div>
				<!--HINTEND:nav-->
				<div class="vertbreak"></div>
				<!--DATE:2023-07-30-->
				<!--HINT:blogpost-->
				<p class="dateHead">30 July 2023</p>
				<article id="analogueVideo">
					<div id="bodyHeader" class="pagehead">
						<!--HINT:posttitle-->
						<h1>Emulating Analogue Video</h1>
						<!--HINTEND:posttitle-->
					</div>
					<img src="../assets/analogue_saccharinemagic.jpg" class="bigImage" title="Emulating Analogue Video"/>
					<p>In the distant past, everything was analogue. That meant instead of representing stuff as a sequence of 1s and 0s, it was represented in a more direct way. Images were recorded directly on film, sounds were recorded as grooves on large disks, and videos were recorded on long reels of film. The next step was to move from the mechanical and chemical methods to the magnetic and electric methods, thus giving us radio and TV, music tapes and video tapes. And then that lead to the digital methods of today. I will be talking about <span class="italic">magnetic and electronic</span> analogue video since I wrote two pieces of software that emulate that, not film. These two pieces of software, <a href="https://github.com/maxotaku11niku/AnalogueConvertEffect">AnalogueConvertEffect</a> and <a href="https://github.com/maxotaku11niku/VideoAnalogiser">VideoAnalogiser</a>, share the same basic principle of directly simulating the signals. But in order to make them, I needed to know how analogue video worked<span class="note">[1]<span>Even if I was more approximate, because otherwise my results wouldn't look right.</span></span>, so what follows is a lengthy write-up about the intricaces of analogue TV and VHS and whatnot. Lettuce begin.</p>
					<h2>Vision from afar</h2>
					<div class="floatRight">
						<img src="../assets/tv_broadcasting_process.png" title="how to send videos to millions of people without the internet" alt="Badly drawn and very simplified drawing of the process of TV broadcasting."/>
						<p>Highly simplified drawing of the process of TV broadcasting</p>
					</div>
					<p>'Television' pretty much literally means 'vision from afar'. It is the culmination of several enabling technologies, and the basis of quite a few others. In order for a practical TV system to have become possible, the following things had to be invented:</p>
					<ul>
						<li>A way to record moving images and make them an electric signal.</li>
						<li>A way to send those signals a long way.</li>
						<li>A way to display those signals as a moving image.</li>
					</ul>
					<p>By the 1920s, all of these were in place as:</p>
					<ul>
						<li>Video recording tubes</li>
						<li>Radio technology/telegraph cables</li>
						<li>Cathode ray tube displays</li>
					</ul>
					<p>Electromechanical TV was trialled first but turned out to be horribly impractical. Mechanical setups had to use big spinning disks with holes poked out in a pattern in order to scan a scene. Getting the sort of resolutions and framerates we're used to today with those things is flat out impossible. Fully electronic TV was what eventually won out, and regular (yet experimental) broadcasts started in the 1930s (over radio waves). So the process has these 3 main steps:</p>
					<ul>
						<li>Record a scene through a camera, scanning in a regular pattern to turn the image into an electric signal.</li>
						<li>Send the signal over modulated radio waves.</li>
						<li>Receive the signal, demodulate it, and display it on a CRT.</li>
					</ul>
					<p>The very first broadcasts were in monochrome as this kept the signals very simple, but after a couple of decades colour TV came, along with suitably more complicated signals. As the aging analogue technology started to put serious limits on resolution and the number of channels that could be broadcast, TV was switched to digital. Digital TV is essentially sending .mp4s over radio waves, to be perfectly honest. No wonder eventually the Internet would come to challenge traditional broadcast TV, but for now let's go back to the beginning.</p>
					<h2>Simple signals</h2>
					<div class="floatRight">
						<img src="../assets/analogue_video_types.png" title="film vs crt, who did it better?" alt="Badly drawn diagram contrasting film video with electronic analogue video."/>
						<p>The biggest difference between film video and electronic analogue video (logically speaking): whether or not there is a discrete number of lines.</p>
					</div>
					<p>So let's think about the fundamentals of video. A full colour video is essentially a 3-dimensional space - of time and 2-dimensional location - where each point has 3-dimensional data associated with it: the 3 axes of colour. In film we have a number of 2-dimensional images, which could be full colour or monochrome. Since we're constrained by the amount of film we can use and how fast we can feed it through a projector, we have to divide our theoretical 'perfect video' into discrete frames. All video does this. Now let's think about a continuous 1-dimensional electric signal: how do we represent an image that way? Well, we'll have to have a number of discrete lines. Traditionally, these are horizontal lines, and although the image is now discretised vertically it's still continuous horizontally (but this does not necessarily mean infinite resolution). So a video is a series of these images, as before. For digital video, the stream is intrinsically discrete, so the horizontal axis has to be discrete as well. Logically speaking, we could imagine a system where we have horizontal and vertical axes being discrete and the time axis being continuous. Such a system wouldn't have a 'frame rate' so it would be very smooth. However this system would be woefully impractical, since you'd have to send a signal for each 'light' on a display. So forget about that sort of display.</p>
					<p>A monochrome analogue TV signal is extremely simple: it just represents brightness. The level of the signal directly corresponds to the brightness level you'd see on the screen... well not quite. Due to the response curves of the display, the signal actually has some gamma correction applied to it in order to reproduce the scene nearly as it was captured through the camera. Additionally, to help with display on TV sets, there are sections of the signal which don't represent anything visible. These sections control syncing behaviour, and there are known periods where the scanning mechanism needs time to readjust to draw the next line or frame. Even then, though, monochrome was just simple, through and through. The cameras were simple and the screens were simple too. But most of us don't see in monochrome so it was inevitable that people would demand colour,<span class="note">[2]<span>Evidently, not everyone. Some people were not so fussed about getting a colour TV back when it was new. Probably because those sets cost quite a bit.</span></span> and this is where things get interesting.</p>
					<h2>Colourised TV</h2>
					<img src="../assets/path_to_composite_video.png" class="bigImage" title="it just kept getting better" alt="A diagram showing the steps to go from naive analogue video to composite video."/>
					<p>The naïve way to do colour TV would simply be to send 3 signals: one for each colour channel, red, green and blue. Certainly the camera must record RGB and the display must output RGB. It seems fine, but it has a major problem: this system is not backwards compatible with monochrome systems. Ah well, just send the monochrome signal as well. But another major problem presents itself: you're now using 4 times as much bandwidth just to add colour to broadcasts without leaving everyone stuck on monochrome behind! Is there a way to not use up so much of the precious radio spectrum just to add colour to TV? Well of course there is... engineers did just that.<span class="note">[3]<span>How else would we have gotten colour TV?</span></span></p>
					<p>First of all, RGB is just one way to parameterise colour. We can transform our RGB axes to another set of axes and still be able to represent all the colours we want. Luminosity (or luma) can be parameterised as some linear combination of R,G and B, so we can demand one of these axes to be luma. We'll call this axis Y for historical reasons. Now to determine the other 2 axes. These 2 axes are purely colour (or chroma) axes, and we still have some freedom in how we choose them. In all colour systems that were used, the axes were both some linear combination of R-Y (red difference) and B-Y (blue difference). These axes are called V and U respectively. Now we can identify Y with the old monochrome signal and we have now got a system which only uses 3 times as much bandwidth... not a great improvement huh.</p>
					<p>But wait! We've gotten an additional benefit! Human vision is less sensitve to changes in chroma than luma. This means we can get away with lower-resolution colour signals! (even digital video does this) Which means we can cut the bandwidth requirements down further. Let's say we can quarter the chroma resolution, then our total signal uses 1.5 times as much bandwidth as the monochrome signal. At this point we're starting to approach parity with the monochrome system. But there's more we can do, and it's these next steps are a bit cheeky.</p>
					<p>First of all there are methods to smoosh<span class="note">[4]<span>The technical term is 'multiplex' but this is funnier.</span></span> the two colour signals into one, bringing us to 1.25 times as much bandwidth as the monochrome signal. I'll elaborate on this soon as this is where all the colour systems differ. And the final step is to realise that the high-frequency end of the luma spectrum is likely to be fairly empty as these small details probably wouldn't have been recorded by the cameras of the time (the 1950s-1960s). So we can get away with smooshing the chroma spectrum within the range of the luma spectrum, such that we use no more of the spectrum than the old monochrome signal. And with only minor artifacts as well (for the time anyway). But engineers seemed to disagree on how best to smoosh the two colour signals together, and now you're gonna see some maybe familiar names crop up.</p>
					<h2>NTSC: America tries first</h2>
					<img src="../assets/ntsc_overview.png" class="bigImage" title="stupid american system" alt="A diagram showing an overview of the NTSC colour system."/>
					<p>NTSC was the first widely-used colour system for analogue video developed. The term 'NTSC' refers to the system developed by the NTSC - National Television System Committee - in the USA, strictly speaking. After failed experiments with a more naïve system, the NTSC decided on a much better system based on the ideas I've already laid out. This system was introduced at the end of 1953 (well before colour broadcasts would become regular). NTSC uses a set of chroma axes that are rotated by 33 degrees relative to the UV axes I've already mentioned. So this system has an I axis (orange-blue) and a Q axis (purple-green) for chroma; these were chosen because human colour vision is more sensitive to changes in the I axis than changes in the Q axis, so Q could be broadcast with even lower bandwidth than I. These components were smooshed into the same bandwidth with <a href="https://en.wikipedia.org/wiki/Quadrature_amplitude_modulation">quadrature amplitude modulation</a>, which explains the name of these axes: I stands for <span class="italic">in-phase</span> and Q stands for <span class="italic">quadrature</span>. Handily, the amplitude of the resulting signal represents saturation, and its phase represents hue.</p>
					<p>Unfortunately it would normally be impossible to figure out the absolute phase of the chroma signal. This is why NTSC signals have a <span class="italic">colourburst</span> in the horizontal blanking period before the start of each line. This burst provides the phase reference so that the proper hue can be obtained from a receiver. But this phase reference can sometimes drift, which leads to the hue of NTSC video in broadcasts drifting towards looking wrong. Which is why in NTSC regions, TV sets often had a hue control to correct for this. And also why many people made fun of it. Which I will now do in the grand old British tradition of making fun of Americans. Oh those silly Americans.</p>
					<p>These days when people refer to NTSC, they usually mean 480p 60 FPS video. This is related to the colour system - as NTSC was almost always used with System M - but not exactly the same. This is in contrast to PAL, which was usually used with 576p 50 FPS video. Now take a wild guess at which system was ultimately the reason for the most common refresh rate in the world...<span class="note">[5]<span>If you're reading this in the far future when 144 FPS is more common, well...</span></span></p>
					<h2>PAL: Fine German engineering</h2>
					<img src="../assets/pal_overview.png" class="bigImage" title="based european system" alt="A diagram showing an overview of the PAL colour system."/>
					<p>European engineers saw problems in NTSC that they wished to try and get around. One of the solutions kept the QAM colour signal, but added a crucial feature to deal with the hue errors. It's in the name: Phase Alternating Line. PAL was unveiled in 1963, having been developed primarily by Telefunken in West Germany<span class="note">[6]<span>"Younger readers, if you think term is weird, google 'cold war germany' on your iPhones..." is what I would say if I was 40 years older than I actually am.</span></span>. PAL has a lot in common with NTSC, such as also using QAM to smoosh the two chroma signals together. PAL uses YUV directly, however, but this is a relatively minor difference compared to its key innovation.</p>
					<div class="floatLeft">
						<img src="../assets/pal_explanation.png" title="look, smartass. it's called pal because of this, not because it's your friend." alt="Badly drawn diagram explaining why PAL's phase alternation cancels out hue errors."/>
						<p>PAL's phase alternation cancels out hue errors by effectively cancelling out phase errors. Note that such phase errors should not vary from line to line, otherwise the logic behind this method to extract the true hue fails.</p>
					</div>
					<p>The phase of the PAL colourburst alternates from line to line, and sign of the V part of the signal alternates too. This important feature can cancel out hue errors resulting from phase errors, and replaces them with a slight desaturation which is less jarring than incorrect hues. This works because if we assume the same phase error applies to normal and phase-reversed lines, and then reverse the phase of the phase-reversed lines, the phase error of the now-normal phase-reversed lines is now itself reversed. Now we can average the normal and reversed phase-reversed signals and the phase errors will cancel out! But the magnitude of the result will be reduced, which corresponds to the reduced saturation. This method also ends up reducing vertical chroma resolution, but that's not a huge deal anyway.</p>
					<p>It would have been possible to combine the YIQ system that NTSC used with the phase-alternation system to get the best of both worlds. So if the engineers knew this, why didn't they go for it? Because, naturally, it would have made it harder to decode. From our perspective it seems trivial to just use a colourspace that is simply rotated from another. But these systems were devised before the integrated circuit was commercialised. The decoding circuitry would have been prohibitive for early public adoption of colour television in PAL regions if they had gone with the YIQ space. Still that wasn't the only possibility...</p>
					<h2>SECAM: The wacky French</h2>
					<img src="../assets/secam_overview.png" class="bigImage" title="weird french system" alt="A diagram showing an overview of the SECAM colour system."/>
					<p>French engineers went for a very different system. When they saw problems with NTSC, they decided those problems were with the modulation method they used. So they devised SECAM (SÉquentiel de Couleur À Mémoire) and it was first proposed in 1961. SECAM uses <a href="https://en.wikipedia.org/wiki/Frequency_modulation">frequency modulation</a> instead of QAM to smoosh the two colour signals together. The reason for using FM is simple: the absolute phase doesn't matter so phase errors can't distort the colour of images! This makes the colourburst unnecessary, by the way. But there is of course a catch: you can't actually just smoosh two signals into one with FM. SECAM smooshes them together by alternating which colour component to send on each line. SECAM uses YDbDr, which is just rescaled YUV, so one line is Db and the next is Dr. So vertical colour resolution is halved in SECAM as well.</p>
					<p>There are a few issues with SECAM, though. The FM subcarrier is obligated to be near-constant amplitude all the time, which means that SECAM signals cannot have true blacks. Additionally, in the era of analogue editing one could not simply add two SECAM signals to combine them the way you can with NTSC and PAL, which made editing SECAM video a pain. Allegedly, part of the reason why the French persisted in developing their own colour system was out of national pride. But that's only a theory...</p>
					<h2>How does it look in the end?</h2>
					<img src="../assets/analogue_convert_comparison.png" class="bigImage" title="i promise you this is 90% accurate" alt="A comparison of the effect of my analogue filter effect on a test image when using all 3 major colour systems."/>
					<p>Given that almighty infodump<span class="note">[7]<span>Go on Wikipedia for more infodumping!</span></span> you might now be wondering how the hell this is supposed to tell you much about what analogue video looks like. I made a little test image to show these, which is shown above. There is, of course, a loss of resolution, though this is because in the analogue age it wasn't so easy to increase picture quality, so the standards were designed with no more than 8 MHz of bandwidth for each channel (and often a bit less). There is colour bleed coming from the lower bandwidth of the colour signal, accentuated by an offset caused by the filtering I've used. Also apparent are various composite video artifacts which come from crosstalk between luma and chroma information. NTSC and PAL exhibit <span class="italic">chroma dots</span>, which are the result of the chroma signal interfering with the luma signal. They also exhibit <span class="italic">rainbow artifacts</span> from the luma signal interfering with the chroma signal. SECAM doesn't exhibit chroma dots, but there are still colour artifacts. This example was made with my AnalogueConvertEffect, and I'm less confident about the magnitude of the colour artifacts with SECAM. This is because I hacked together a very crude method of FM demodulation, unlike my cleaner QAM demodulation code. You can also see the striped patterns on the side become uniform in PAL and SECAM but not in NTSC: this is a result of phase/component alternation.</p>
					<p>This example here is "clean". That means there is no noise, the filtering is pretty decent and that's why it doesn't quite look totally analogue in the same way as a cheap VHS filter. VHS uses a lower bandwidth for luma and chroma than broadcast TV<span class="note">[8]<span>I implicitly mean analogue broadcasts, which have stopped in most countries now. So is that even true anymore?</span></span> so VHS video looks lower quality than this. VHS also modulates in chroma and luma quite differently to broadcast standards, putting the luma signal through FM and the chroma signal through AM. VHS, due to having data on a tape, also exhibits different artifacts to broadcast analogue signals. I have not yet programmed in tape-specific artifacts to any of my things, since I'm going to try and understand where they come from. But I do know that a particular VHS filter (which does not fully simulate an analogue signal) might just be overlaying canned footage of VHS artifacts. To be honest, that's probably fine enough for most purposes. In any case, I will present a dirtier<span class="note">[9]<span>NOT LIKE THAT</span></span> example (This is a PAL image specifically because I'm from Europe):</p>
					<img src="../assets/analoguetest_dirty.jpg" class="bigImage" title="you wanted more analogue? here you go" alt="A more noisy and artifact ridden analogue image example."/>
					<h2>The Making of AnalogueConvertEffect</h2>
					<p>To end, I'm going to give a brief overview of my thoughts when I made the two things that I mentioned at the start. AnalogueConvertEffect was to be the way that I would make still images look like stills from analogue broadcasts. I envisioned it as being a plugin, and I have so far made it work in Paint.NET, although now that I have developed it, it shouldn't be too much hassle to port it to other programs. I wanted to make it for fun, and also because I wanted a way to make images that look 'analogue' and wanted to do that in a realistic way. So the process is simple: convert a digital image to a proper analogue signal, represented as an array of floating point numbers. Then transform that signal (add noise, distortion etc.) and decode it to a digital image again. Rescaling was necessary since the standards require a certain number of scanlines, but I initially left the width unchanged.</p>
					<p>Encoding was easy. I referenced an ITU document on analogue broadcast standards for all the information necessary, which included equations for the signals. So I could just rescale the image, then sample and transform the colours to a YUV-type space, then make a signal. Piece of cake. I decided to leave out the sync pulse and colourburst since my decoder wasn't going to full-on simulate a decoding circuit and these components were in the invisible region, far enough away from the visible region to ignore. Transforming the signal would be easy too, but after that I had to decode a signal. First, I simply treated it as an unfiltered monochrome signal as a sanity check, and got a greyscale image with a dotty pattern on it, as expected. I decided to do PAL first simply because PAL is the best. Now I had to extract and demodulate the subcarrier and filter the signal, which was the hard part. I was a bit stumped on what to do, so first of all I decided to Fourier transform the signal to get its spectrum then filter it simply by multiplication (instead of... convolution, which would be... slow). After all the filtering was finished then I would inverse Fourier transform and get my signal out.</p>
					<p>The Fourier transform method was a pain to make. I decided to waste time writing an FFT algorithm, then it turned out my chroma decoding was off and causing weird patterning, which required a kludge to fix. And of course it was painfully slow to process images. There had to be a better way to do this, and there was. Turned out that convolution, in a way, was better. Using finite impluse response (FIR) filters, which act directly on the signal, I could generate images much faster, especially after using a fixed width for rescaling as, of course, bandwidth limiting would lower the image quality anyway. It was also a more 'realistic' approach, as real filters don't transform signals into spectrums in order to filter them. So then I could reimplement decoding for all systems (I had done basic implementation earlier on) which would go faster and look better. The process of developing this plugin required a lot of testing, so I had to keep opening and closing Paint.NET every time I wanted to see the effects of my code changes. That was fun.<span class="note">[10]<span>It wasn't</span></span></p>
					<h2>The Making of VideoAnalogiser</h2>
					<p>During the development of AnalogueConvertEffect, I had the idea to extend its function to videos. For a while I thought 'oh this is going to be a pain, let's not do this actually, can we just take a break?' Then I decided to do it anyway. I copied code over from AnalogueConvertEffect and 98VideoConverter<span class="note">[11]<span>Although the previous blog post deals with this program, I have significantly updated both 98VIDEOP.COM and 98VideoConverter since then, which makes them both much better than before.</span></span>, used the FFmpeg libraries again and got to work. I made it a console application in C++, not a plugin, because I wanted to fix the framerate as well (a plugin may not be able to enforce this). And because I don't know how to make plugins for the video editor I use. And also because I didn't want to make another Win32 program, for both portability and sanity reasons.</p>
					<p>VideoAnalogiser's development mainly focused on getting modern digital videos out of the processed frames. It's not worth talking much about the process as it's just wrangling with the FFmpeg libraries' file output and encoding functions. Importantly, though, VideoAnalogiser does not generate the signal all at once. This is because it would rapidly exhaust memory. In order to represent the signal at double precision with a 6 MHz bandwidth, we would need a sampling rate of 12 MHz (by the <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist–Shannon sampling theorem</a>) and therefore a data rate of <span class="italic"><span class="bold">96 MB/s!</span></span> I would have exhausted my 8 GB of RAM with just <span class="italic">85 seconds of video!</span> It worse than that since other programs need to use RAM too. Of course, double precision is overkill for this, but either way I do the conversion frame by frame. I also force interlacing to be on since analogue video was typically interlaced.</p>
					<p>The audio is not yet processed by VideoAnalogiser. I'm going to see whether I should modulate it into the signal or just filter it. Should probably do the former for accuracy, but then it would have to be FM. My FM decoder is a massive kludgefest, after all. Maybe SECAM really is just like that, and I should really just prefilter the colour signal? Anyway, VideoAnalogiser is still new and has room for all sorts of improvements, so stay tuned.<span class="note">[12]<span>See this is clever because I've been talking about analogue TV and you had to actually tune the TV manually and... whatever it's lost its original meaning anyway I dunno why I bother...</span></span></p>
					<h2>Foregone Conclusion</h2>
					<p>The end of the analogue era was inevitable. The objective quality of analogue media eventually hit its limits: that's how it goes, new technology displaces old. But if the vinyl revival is anything to go by, people can still be attached to the old technology. As all existing VHS tapes and VCRs go kaput one by one, the only way left to see what analogue video looked like are the digitisations of old recordings, and simulations like the ones I've programmed. Ultimately, this era of video had an impact on modern digital video. As I've mentioned before, modern digital video still tends to reduce the resolution of chroma information to save space. Just goes to show you that a good idea really can last a long time.</p>
					<p>So these programs really took up quite a bit of my time, and this is all I have to show for it, huh. Well, perhaps this is an opportunity to get them even closer to accuracy. Anyway that's it for now.</p>
				</article>
				<!--HINTEND:blogpost-->
			</div>
			<footer id="commonFooter">
				<p>2023 Maxim Hoxha</p>
			</footer>
		</div>
		<script src="../scripts/menuopen.js"></script>
	</body>
</html>